{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.2.implementation_of_multilayer_perceptrons_from_scratch_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMKV8VY1T56JsU6QYHOdOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/4.multilayer_perceptrons/4_2_implementation_of_multilayer_perceptrons_from_scratch_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Wup5IDEgu0Hh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    # Divide all numbers by 255 so that all pixel values are between\n",
        "    # 0 and 1, add a batch dimension at the last. And cast label to int32\n",
        "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
        "                            tf.cast(y, dtype='int32'))\n",
        "    resize_fn = lambda X, y: (\n",
        "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
        "    return (\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
        "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
        "            batch_size).map(resize_fn))"
      ],
      "metadata": {
        "id": "h84RO19QvU1Q"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "iuen7dpMvFDB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
        "W1 = tf.Variable(tf.random.normal(\n",
        "    shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))\n",
        "b1 = tf.Variable(tf.zeros(num_hiddens))\n",
        "W2 = tf.Variable(tf.random.normal(\n",
        "    shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))\n",
        "b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))\n",
        "\n",
        "params = [W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "-ukpJ9gWwLAC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(X):\n",
        "    return tf.math.maximum(X, 0)"
      ],
      "metadata": {
        "id": "9KireE_zwh17"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X):\n",
        "  X = tf.reshape(X, (-1, num_inputs))\n",
        "  H = relu(tf.matmul(X, W1) + b1)\n",
        "  return tf.matmul(H, W2) + b2"
      ],
      "metadata": {
        "id": "RpmZXUXHw5gl"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_hat, y):\n",
        "  return tf.losses.sparse_categorical_crossentropy(y, y_hat, from_logits=True)"
      ],
      "metadata": {
        "id": "Xzc3nOcYxyhG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
      ],
      "metadata": {
        "id": "1OWM_3i2yODt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(net, data_iter):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), len(y))\n",
        "    return metric[0] / metric[1]"
      ],
      "metadata": {
        "id": "KWioZfbfyOkR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "tnjmzxQ5yQSx"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch3(net, train_iter, loss, updater, params, lr):\n",
        "  metric = Accumulator(3)\n",
        "  for X, y in train_iter:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = net(X)\n",
        "      l = loss(y_hat, y)\n",
        "    \n",
        "    updater(X.shape[0], tape.gradient(l, params), params, lr)\n",
        "    l_sum = tf.reduce_sum(l)\n",
        "    metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "  return metric[0] / metric[2], metric[1] / metric[2]"
      ],
      "metadata": {
        "id": "AF0bcf8xyWy2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater, params, lr):\n",
        "  for epoch in range(num_epochs):\n",
        "    train_metrics = train_epoch_ch3(net, train_iter, loss, updater, params, lr)\n",
        "    test_acc = evaluate_accuracy(net, test_iter)\n",
        "    print('epoch: %s' % epoch)\n",
        "    print(train_metrics)\n",
        "    print(test_acc)\n",
        "  train_loss, train_acc = train_metrics"
      ],
      "metadata": {
        "id": "hPRNuA-3yXwt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, grads, lr, batch_size):\n",
        "    for param, grad in zip(params, grads):\n",
        "        param.assign_sub(lr*grad/batch_size)"
      ],
      "metadata": {
        "id": "f5Cci2YTygkz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def updater(batch_size, grads, params, lr): \n",
        "  sgd(params, grads, lr, batch_size)"
      ],
      "metadata": {
        "id": "z2_3fnODykeh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 10, 0.1\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, updater, [W1, W2, b1, b2], lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsUYNorzyB0n",
        "outputId": "12aba28b-c31e-431d-fb87-1b6662a0678e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "(1.0390050524393717, 0.642)\n",
            "0.7292\n",
            "epoch: 1\n",
            "(0.5978441931406657, 0.7899666666666667)\n",
            "0.7968\n",
            "epoch: 2\n",
            "(0.5178074743906657, 0.81875)\n",
            "0.8213\n",
            "epoch: 3\n",
            "(0.47877785797119143, 0.83305)\n",
            "0.8128\n",
            "epoch: 4\n",
            "(0.45603862469991047, 0.8387333333333333)\n",
            "0.8338\n",
            "epoch: 5\n",
            "(0.43227490488688153, 0.84765)\n",
            "0.8396\n",
            "epoch: 6\n",
            "(0.41748050651550295, 0.8532166666666666)\n",
            "0.8382\n",
            "epoch: 7\n",
            "(0.4036259128570557, 0.8585666666666667)\n",
            "0.8495\n",
            "epoch: 8\n",
            "(0.39335877742767333, 0.8598333333333333)\n",
            "0.8528\n",
            "epoch: 9\n",
            "(0.3802210620880127, 0.8654166666666666)\n",
            "0.8539\n"
          ]
        }
      ]
    }
  ]
}
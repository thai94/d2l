{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.3.concise_implementation_of_multilayer_perceptrons_tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMv5sp/b966Ln8A9zDSL4fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/4.multilayer_perceptrons/4_3_concise_implementation_of_multilayer_perceptrons_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lTYs_le3b4sH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)])"
      ],
      "metadata": {
        "id": "J_DxjTcnb6AQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    # Divide all numbers by 255 so that all pixel values are between\n",
        "    # 0 and 1, add a batch dimension at the last. And cast label to int32\n",
        "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
        "                            tf.cast(y, dtype='int32'))\n",
        "    resize_fn = lambda X, y: (\n",
        "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
        "    return (\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
        "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
        "            batch_size).map(resize_fn))\n",
        "\n",
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))\n",
        "\n",
        "def evaluate_accuracy(net, data_iter):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), len(y))\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def train_epoch_ch3(net, train_iter, loss, updater, params, lr):\n",
        "  metric = Accumulator(3)\n",
        "  for X, y in train_iter:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = net(X)\n",
        "      if isinstance(loss, tf.keras.losses.Loss):\n",
        "        l = loss(y, y_hat)\n",
        "      else:\n",
        "        l = loss(y_hat, y)\n",
        "\n",
        "    if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
        "      params = net.trainable_variables\n",
        "      grads = tape.gradient(l, params)\n",
        "      updater.apply_gradients(zip(grads, params))\n",
        "    else:\n",
        "      updater(X.shape[0], tape.gradient(l, params), params, lr)\n",
        "    l_sum = tf.reduce_sum(l)\n",
        "    metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "  return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater, params, lr):\n",
        "  for epoch in range(num_epochs):\n",
        "    train_metrics = train_epoch_ch3(net, train_iter, loss, updater, params, lr)\n",
        "    test_acc = evaluate_accuracy(net, test_iter)\n",
        "    print('epoch: %s' % epoch)\n",
        "    print(train_metrics)\n",
        "    print(test_acc)\n",
        "  train_loss, train_acc = train_metrics"
      ],
      "metadata": {
        "id": "nmg0YxebcAUK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, lr, num_epochs = 256, 0.1, 10\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "trainer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer, None, lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h0Fq741b8G8",
        "outputId": "5c92e39f-0168-4465-a10a-3462f30cd5c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "(0.002883693205813567, 0.7562833333333333)\n",
            "0.8011\n",
            "epoch: 1\n",
            "(0.0020279876967271167, 0.8216)\n",
            "0.8171\n",
            "epoch: 2\n",
            "(0.0018227596312761307, 0.83775)\n",
            "0.8311\n",
            "epoch: 3\n",
            "(0.0016962133139371872, 0.8480333333333333)\n",
            "0.8382\n",
            "epoch: 4\n",
            "(0.0016101967359582582, 0.85555)\n",
            "0.8416\n",
            "epoch: 5\n",
            "(0.0015577166840434074, 0.85985)\n",
            "0.8474\n",
            "epoch: 6\n",
            "(0.0014895240172743797, 0.86695)\n",
            "0.8308\n",
            "epoch: 7\n",
            "(0.0014492553025484084, 0.8690833333333333)\n",
            "0.8562\n",
            "epoch: 8\n",
            "(0.0014141719241937002, 0.8733166666666666)\n",
            "0.8574\n",
            "epoch: 9\n",
            "(0.0013772437175114949, 0.8750166666666667)\n",
            "0.8627\n"
          ]
        }
      ]
    }
  ]
}
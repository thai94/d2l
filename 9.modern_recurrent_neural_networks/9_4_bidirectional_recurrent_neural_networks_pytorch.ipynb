{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.4.bidirectional_recurrent_neural_networks_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM70OcBYbAbyEICatlAfaUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/9.modern_recurrent_neural_networks/9_4_bidirectional_recurrent_neural_networks_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WzY_syb7aJlS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "\n",
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "\n",
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)\n",
        "\n",
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def tokenize(lines, token='word'):\n",
        "\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == 'char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print('ERROR: unknow token type: ' + token)\n",
        "\n",
        "def count_corpus(tokens):\n",
        "\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "   tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "    counter = count_corpus(tokens)\n",
        "\n",
        "    self._token_freqs = sorted(counter.items(), key = lambda x: x[1], reverse=True)\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break;\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "  \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    else:\n",
        "      return [self.to_tokens(idx) for idx in indices]\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    return 0\n",
        "\n",
        "  @property\n",
        "  def token_freqs(self):\n",
        "    return self._token_freqs\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "\n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, 'char')\n",
        "  vocab = Vocab(tokens)\n",
        "\n",
        "  corpus = [vocab[token] for line in tokens for token in line]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens]\n",
        "  return corpus, vocab\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "\n",
        "  corpos = corpus[random.randint(0, num_steps - 1):]\n",
        "  num_subseqs = (len(corpos) - 1) // num_steps\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "    return corpus[pos: pos + num_steps]\n",
        "  \n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(idx) for idx in initial_indices_per_batch]\n",
        "    Y = [data(idx + 1) for idx in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps]\n",
        "    yield X, Y\n",
        "\n",
        "class SeqDataLoader:\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "def load_data_time_machine(batch_size, num_steps,\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLTbqDClaO5I",
        "outputId": "4452abf0-cc11-4264-eab5-83596979abec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps, device = 32, 35, try_gpu()\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "Y_J0SHdtaO8X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNNModel, self).__init__(**kwargs)\n",
        "\n",
        "    self.rnn = rnn_layer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_hiddens = self.rnn.hidden_size\n",
        "    if not self.rnn.bidirectional:\n",
        "      self.num_directions = 1\n",
        "      self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "    else:\n",
        "      self.num_directions = 2\n",
        "      self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "  \n",
        "  def forward(self, inputs, state):\n",
        "\n",
        "    X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "    X = X.to(torch.float32)\n",
        "    Y, state = self.rnn(X, state)\n",
        "    output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "    return output, state\n",
        "\n",
        "  def begin_state(self, device, batch_size=1):\n",
        "    if not isinstance(self.rnn, nn.LSTM):\n",
        "      return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                device=device)\n",
        "    else:\n",
        "      return (torch.zeros((\n",
        "                self.num_directions * self.rnn.num_layers,\n",
        "                batch_size, self.num_hiddens), device=device),\n",
        "                    torch.zeros((\n",
        "                        self.num_directions * self.rnn.num_layers,\n",
        "                        batch_size, self.num_hiddens), device=device))\n",
        "      \n",
        "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
        "\n",
        "  state = net.begin_state(batch_size=1, device= device)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "  for y in prefix[1:]:\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  \n",
        "  for _ in range(num_preds):\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
        "\n",
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()\n",
        "\n",
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "\n",
        "  state, timer = None, Timer()\n",
        "  metric = Accumulator(2)\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        state.detach_()\n",
        "      else:\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater(batch_size=1)\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
        "\n",
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "  \n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
        "  \n",
        "  predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "\n",
        "  ui_x = []\n",
        "  ui_y = []\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch_ch8(\n",
        "            net, train_iter, loss, updater, device, use_random_iter)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(predict('time traveller'))\n",
        "      ui_x.append(epoch + 1)\n",
        "      ui_y.append(ppl)\n",
        "\n",
        "  \n",
        "  print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))\n",
        "\n",
        "  plt.plot(ui_x, ui_y, 'r')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MG_OMipwbDJj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
        "num_inputs = vocab_size\n",
        "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)\n",
        "model = RNNModel(lstm_layer, len(vocab))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "keQndxFYa2c5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y8OULcMbbSmz",
        "outputId": "a40d8031-7a5d-4f03-805f-356542351f93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller                                                  \n",
            "time traveller                                                  \n",
            "time travellereeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "time travellereeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "time travellereeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "time travellerererererererererererererererererererererererererer\n",
            "perplexity 1.1, 125008.6 tokens/sec on cuda:0\n",
            "time travellerererererererererererererererererererererererererer\n",
            "travellerererererererererererererererererererererererererer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.7.concise_implementation_of_softmax_regression_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSCw7ionHp9IVGYzgJTCkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/3.linear-neural-networks/3_7_concise_implementation_of_softmax_regression_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CollJaSa4frG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None):\n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    # Divide all numbers by 255 so that all pixel values are between\n",
        "    # 0 and 1, add a batch dimension at the last. And cast label to int32\n",
        "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
        "                            tf.cast(y, dtype='int32'))\n",
        "    resize_fn = lambda X, y: (\n",
        "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
        "    return (\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
        "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
        "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
        "            batch_size).map(resize_fn))"
      ],
      "metadata": {
        "id": "mei4fcqb5Biw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "2fNYbNoZ48A0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential()\n",
        "net.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
        "net.add(tf.keras.layers.Dense(10, kernel_initializer=weight_initializer))"
      ],
      "metadata": {
        "id": "mvLYW9zn5G0Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "trainer = tf.keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "3ISlEuQN7Nuz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
      ],
      "metadata": {
        "id": "bkisjBlc7yvx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(net, data_iter):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), len(y))\n",
        "    return metric[0] / metric[1]"
      ],
      "metadata": {
        "id": "h4MJetRs75Bn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "3MtZk9TQ77Wu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch3(net, train_iter, loss, updater, params, lr):\n",
        "  metric = Accumulator(3)\n",
        "  for X, y in train_iter:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_hat = net(X)\n",
        "      if isinstance(loss, tf.keras.losses.Loss):\n",
        "        l = loss(y, y_hat)\n",
        "      else:\n",
        "        l = loss(y_hat, y)\n",
        "\n",
        "    if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
        "      params = net.trainable_variables\n",
        "      grads = tape.gradient(l, params)\n",
        "      updater.apply_gradients(zip(grads, params))\n",
        "    else:\n",
        "      updater(X.shape[0], tape.gradient(l, params), params, lr)\n",
        "    l_sum = tf.reduce_sum(l)\n",
        "    metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "  return metric[0] / metric[2], metric[1] / metric[2]"
      ],
      "metadata": {
        "id": "DFTMhQ5G79Jh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater, params, lr):\n",
        "  for epoch in range(num_epochs):\n",
        "    train_metrics = train_epoch_ch3(net, train_iter, loss, updater, params, lr)\n",
        "    test_acc = evaluate_accuracy(net, test_iter)\n",
        "    print('epoch: %s' % epoch)\n",
        "    print(train_metrics)\n",
        "    print(test_acc)\n",
        "  train_loss, train_acc = train_metrics"
      ],
      "metadata": {
        "id": "A81B-wY17_HU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "lr = 0.01\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer, None, lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5MsWX7i8BAe",
        "outputId": "1ec2eb89-03bf-4aeb-9af2-3bdc5c1463a5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "(0.003080140237013499, 0.7473333333333333)\n",
            "0.7924\n",
            "epoch: 1\n",
            "(0.0022357578267653785, 0.8120833333333334)\n",
            "0.8136\n",
            "epoch: 2\n",
            "(0.0020538648664951325, 0.8266833333333333)\n",
            "0.8205\n",
            "epoch: 3\n",
            "(0.0019628437598546346, 0.8315833333333333)\n",
            "0.8205\n",
            "epoch: 4\n",
            "(0.0018993083328008652, 0.8366)\n",
            "0.8253\n",
            "epoch: 5\n",
            "(0.0018540361866354943, 0.84025)\n",
            "0.83\n",
            "epoch: 6\n",
            "(0.0018206486160556475, 0.8433333333333334)\n",
            "0.8247\n",
            "epoch: 7\n",
            "(0.001792177265882492, 0.8460666666666666)\n",
            "0.8305\n",
            "epoch: 8\n",
            "(0.0017696840226650238, 0.8470166666666666)\n",
            "0.8326\n",
            "epoch: 9\n",
            "(0.001750953158736229, 0.8483833333333334)\n",
            "0.8346\n",
            "epoch: 10\n",
            "(0.0017347917646169662, 0.8496333333333334)\n",
            "0.8344\n",
            "epoch: 11\n",
            "(0.0017190943916638693, 0.8502)\n",
            "0.836\n",
            "epoch: 12\n",
            "(0.0017068222537636756, 0.8511833333333333)\n",
            "0.8349\n",
            "epoch: 13\n",
            "(0.0016964888880650203, 0.85225)\n",
            "0.8373\n",
            "epoch: 14\n",
            "(0.0016826957042018573, 0.8539)\n",
            "0.8386\n",
            "epoch: 15\n",
            "(0.001676203283170859, 0.8533)\n",
            "0.8386\n",
            "epoch: 16\n",
            "(0.0016683359781901042, 0.8542)\n",
            "0.8396\n",
            "epoch: 17\n",
            "(0.0016559156447649003, 0.8559)\n",
            "0.8395\n",
            "epoch: 18\n",
            "(0.001648877160747846, 0.85645)\n",
            "0.8387\n",
            "epoch: 19\n",
            "(0.0016463133489092192, 0.8565833333333334)\n",
            "0.8392\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.1.layers_and_blocks_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxAAVTHS67OCzOeEtAPNi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/5.deep_learning_computation/5_1_layers_and_blocks_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "jJDIy5JgE7ZB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "lXAxbDSd_P-t"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = nn.Linear(20, 256)\n",
        "    self.out = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.out(F.relu(self.hidden(X)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2, 20)"
      ],
      "metadata": {
        "id": "-NmCw8wycTMD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZuMOCq6wVeY",
        "outputId": "4c869819-37d6-451b-f426-bda9955b0f39"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP()"
      ],
      "metadata": {
        "id": "vCucllz_wpBq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = net(X)"
      ],
      "metadata": {
        "id": "tk0cWUNcwsHE"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGvyyeV0w9vv",
        "outputId": "c53ba8a6-08d5-4164-d566-f73a286ff203"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2561, -0.0616,  0.2754,  0.0986,  0.0298, -0.0977, -0.1810,  0.3225,\n",
              "         -0.0349,  0.1721],\n",
              "        [ 0.3375,  0.0198,  0.1613,  0.1531,  0.1566, -0.0940, -0.2711,  0.2084,\n",
              "          0.0517,  0.0891]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "    super().__init__()\n",
        "    for idx, module in enumerate(args):\n",
        "      self._modules[str(idx)] = module\n",
        "  \n",
        "  def forward(self, X):\n",
        "    for block in self._modules.values():\n",
        "      X = block(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "VaafvdqdxEtT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_8VT48-0fFN",
        "outputId": "c9075cc5-85c1-489f-818f-25403e53f375"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0981, -0.0491,  0.0119,  0.0562, -0.0078, -0.0514, -0.0332,  0.0052,\n",
              "          0.2526,  0.0752],\n",
              "        [ 0.0169, -0.1741,  0.0738, -0.0126, -0.0145,  0.0655,  0.1295,  0.0722,\n",
              "          0.0398,  0.0618]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
        "    self.linear = nn.Linear(20, 20)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    X = self.linear(X)\n",
        "    X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
        "    X = self.linear(X)\n",
        "    while X.abs().sum() > 1:\n",
        "      X /= 2\n",
        "    return X.sum()"
      ],
      "metadata": {
        "id": "lKZ6LZLh2zzp"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4HSfATl4biO",
        "outputId": "a25885bb-37d3-4af4-e9be-5f1c17144741"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.3538, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NestMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
        "                                 nn.Linear(64, 32), nn.ReLU())\n",
        "    self.linear = nn.Linear(32, 16)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.linear(self.net(X))"
      ],
      "metadata": {
        "id": "HmT29hPb7Eb1"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())"
      ],
      "metadata": {
        "id": "jUAlEmO38INs"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chimera(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMj36D0h8NmM",
        "outputId": "f6fdb976-ebe7-4dd6-b7d3-d3d6377cd741"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1569, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.6.concise_implementation_of_recurrent_neural_networks_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzZJ8oaZvFNtY0SqT30u/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/8.recurrent_neural_networks/8_6_concise_implementation_of_recurrent_neural_networks_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4-f3Ulh9a9Gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
      ],
      "metadata": {
        "id": "W5Ej74P6bo1P"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')"
      ],
      "metadata": {
        "id": "OhR5P5WXbnfe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname"
      ],
      "metadata": {
        "id": "2lYieN-mbmDt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)"
      ],
      "metadata": {
        "id": "iBUQQpFIbkct"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTz5rtUhbjGN",
        "outputId": "8e5da0c8-ed75-4f58-c5b1-acb02c213116"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')"
      ],
      "metadata": {
        "id": "aMo2WxwTbhe3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):\n",
        "\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == 'char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print('ERROR: unknow token type: ' + token)"
      ],
      "metadata": {
        "id": "17L_r1rkbfvm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_corpus(tokens):\n",
        "\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "   tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "KQ-bsLuJbeTT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "    counter = count_corpus(tokens)\n",
        "\n",
        "    self._token_freqs = sorted(counter.items(), key = lambda x: x[1], reverse=True)\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break;\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "  \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    else:\n",
        "      return [self.to_tokens(idx) for idx in indices]\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    return 0\n",
        "\n",
        "  @property\n",
        "  def token_freqs(self):\n",
        "    return self._token_freqs"
      ],
      "metadata": {
        "id": "DWkPUnYYbb8L"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "\n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, 'char')\n",
        "  vocab = Vocab(tokens)\n",
        "\n",
        "  corpus = [vocab[token] for line in tokens for token in line]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens]\n",
        "  return corpus, vocab"
      ],
      "metadata": {
        "id": "aIUWfHVxbaRo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "\n",
        "  corpos = corpus[random.randint(0, num_steps - 1):]\n",
        "  num_subseqs = (len(corpos) - 1) // num_steps\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "    return corpus[pos: pos + num_steps]\n",
        "  \n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(idx) for idx in initial_indices_per_batch]\n",
        "    Y = [data(idx + 1) for idx in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "4EdgrKRLbYXH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps]\n",
        "    yield X, Y"
      ],
      "metadata": {
        "id": "5qOQ9MH4bVgp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "sjjJDZDFbTQA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_time_machine(batch_size, num_steps,\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "d-GmRS2QbNbg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "rVK8brNbbIeb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
      ],
      "metadata": {
        "id": "NmR3P-uFbtOj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.zeros((1, batch_size, num_hiddens))\n",
        "state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4aa-o9acZmL",
        "outputId": "a3cebef4-ba0b-4aad-ccca-760d772e71c8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(num_steps, batch_size, len(vocab)))"
      ],
      "metadata": {
        "id": "OVDmhayAca12"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, state_new = rnn_layer(X, state)"
      ],
      "metadata": {
        "id": "Xlc2_QfFcfIL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape, state_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl1qNRQ4-QaU",
        "outputId": "bc628173-83b3-4e97-c481-736cc7c61b1b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNNModel, self).__init__(**kwargs)\n",
        "\n",
        "    self.rnn = rnn_layer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_hiddens = self.rnn.hidden_size\n",
        "    if not self.rnn.bidirectional:\n",
        "      self.num_directions = 1\n",
        "      self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "    else:\n",
        "      self.num_directions = 2\n",
        "      self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "  \n",
        "  def forward(self, inputs, state):\n",
        "\n",
        "    X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "    X = X.to(torch.float32)\n",
        "    Y, state = self.rnn(X, state)\n",
        "    output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "    return output, state\n",
        "\n",
        "  def begin_state(self, device, batch_size=1):\n",
        "    if not isinstance(self.rnn, nn.LSTM):\n",
        "      return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                device=device)\n",
        "    else:\n",
        "      return (torch.zeros((\n",
        "                self.num_directions * self.rnn.num_layers,\n",
        "                batch_size, self.num_hiddens), device=device),\n",
        "                    torch.zeros((\n",
        "                        self.num_directions * self.rnn.num_layers,\n",
        "                        batch_size, self.num_hiddens), device=device))\n"
      ],
      "metadata": {
        "id": "_xijwpMS-bhU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
        "\n",
        "  state = net.begin_state(batch_size=1, device= device)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "  for y in prefix[1:]:\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  \n",
        "  for _ in range(num_preds):\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "vSjdDFb-EEuY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = try_gpu()\n",
        "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "net = net.to(device)\n",
        "predict_ch8('time traveller', 10, net, vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hy8SgA0YD_TW",
        "outputId": "05338f4c-4181-4732-e11c-1c23460a5ee5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time travellerfjffyyyjff'"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ],
      "metadata": {
        "id": "GYyliycBERPK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "metadata": {
        "id": "0oWZ3eLnETVR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "wyMZlMZjEU-l"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()"
      ],
      "metadata": {
        "id": "tghXrSujEWWT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "\n",
        "  state, timer = None, Timer()\n",
        "  metric = Accumulator(2)\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        state.detach_()\n",
        "      else:\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater(batch_size=1)\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "bByK-nX2EXzp"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "  \n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
        "  \n",
        "  predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "\n",
        "  ui_x = []\n",
        "  ui_y = []\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch_ch8(\n",
        "            net, train_iter, loss, updater, device, use_random_iter)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(predict('time traveller'))\n",
        "      ui_x.append(epoch + 1)\n",
        "      ui_y.append(ppl)\n",
        "\n",
        "  \n",
        "  print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))\n",
        "\n",
        "  plt.plot(ui_x, ui_y, 'r')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qkAx4Uj0Eau4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PkZrk1kmEKds",
        "outputId": "ac414857-d30b-4f44-fe33-8ae96b2d3b2d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller an the the the the the the the the the the the th\n",
            "time traveller and the the that therering that and the the the t\n",
            "time traveller the thing the thing the thing the thing the thing\n",
            "time traveller the this the thing the thing the this the thing t\n",
            "time travellereatlene the the thase the the thate the the thate \n",
            "time traveller thith siment on the this the mad that le prould f\n",
            "time traveller thicg the gime traveller thicg the gime traveller\n",
            "time traveller and ffre thing that be tha fired thicklest and is\n",
            "time traveller tare man at filler the ght lighte scather and sow\n",
            "time traveller of che ston thas ouls oonth three alathered can t\n",
            "time traveller ffremenoc wasdof thate and and they this way has \n",
            "time traveller of chithe beas thin thane will sonn thes think th\n",
            "time traveller cat of spacingle th ans and the gery ingwtonn gst\n",
            "time traveller ffter as it the gromet mat i buthe e sige that us\n",
            "time traveller of cald the time traveller hat and havele ge thac\n",
            "time traveller aro that ir meling fir pred the payconcumuree s a\n",
            "time traveller fot sagd the orive thewlyor couk thatteen mo ns m\n",
            "time traveller as fourhe rome dit taine so nead a or ano theneth\n",
            "time travelleryou can show braccenseon the rone andthent tes ghe\n",
            "time traveller cof abmer mow s menneard and thite this lithes un\n",
            "time traveller suid fily r aling this le pare are ay hon some ti\n",
            "time traveller artee the s me that arime terrement one mentiensc\n",
            "time travellerit s against reason stil ably romovemavithe there \n",
            "time traveller sathee so l aver thas is ally ack tracee this ing\n",
            "time traveller but now yan a sine thars which saitht obe on tome\n",
            "time traveller but nit you and tract tis patter than submitted t\n",
            "time traveller but now you begincustely so machis tall have thin\n",
            "time traveller proceeded anyreal body must have extensis meant o\n",
            "time traveller for sope tions win thou havi gely why here wracte\n",
            "time traveller ard and sor derand shot at we move aboul on timin\n",
            "time traveller for so it wely undedthey haight roug hin saif the\n",
            "time travellerit s against reason said the medical man there was\n",
            "time travelleryou sand terea sacteatthe doml th there was thatlu\n",
            "time traveller held in his tant ouman thing tine sore as ingenot\n",
            "time travellery al now her aigenstoncen s a lly the eadith op ca\n",
            "time traveller sat along to thest a shall brea tomen some prove \n",
            "time travellerit would be remarkable come fiomether vimingsefira\n",
            "time traveller heme in whishiss moun ab in thitend so eplecentll\n",
            "time traveller after the pauserequly cas of limgind whos and hai\n",
            "time traveller held in his hand was a glitteringmetallic framexo\n",
            "time traveller for se weone bus tout that ly the dime in ine bla\n",
            "time traveller sathal sola alo aacalaig that is the furuthent tr\n",
            "time traveller ffrer the predors th aranithere ara linthan there\n",
            "time traveller and nither oo ig astone hest and as our lay toree\n",
            "time traveller but now you begin to seathe falt tame hivenerene \n",
            "time travellerit fouman se his expothed that ad hest real dishis\n",
            "time traveller peoce ter eimer aly an be and dimention aront as \n",
            "time traveller proceeded any ravitat and havene thm wall trandis\n",
            "time traveller and diand winct muchave ating to apeipte cust you\n",
            "time traveller held in time aravelleryou can show black is white\n",
            "perplexity 1.3, 107679.4 tokens/sec on cuda:0\n",
            "time traveller held in time aravelleryou can show black is white\n",
            "traveller and the e we boue abd anoutwe tofuly have bot so \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
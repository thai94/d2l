{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.5.implementation_of_recurrent_neural_networks_from_scratch_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT79gc/LnoAOEuz8oiqzjy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/8.recurrent_neural_networks/8_5_implementation_of_recurrent_neural_networks_from_scratch_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI-GpLmXDMWh"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'"
      ],
      "metadata": {
        "id": "bom6Ow1wDtw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')"
      ],
      "metadata": {
        "id": "O2IxYKPqDwh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname"
      ],
      "metadata": {
        "id": "sk-g8WX8DxDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)"
      ],
      "metadata": {
        "id": "Rqgrr_l6Dyji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZewHgr99D1r7",
        "outputId": "1fbf77eb-8e06-4915-f13f-73f4f9ec12df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "GoRcmkZo1cjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):\n",
        "\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == 'char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print('ERROR: unknow token type: ' + token)"
      ],
      "metadata": {
        "id": "ucAkp1TGNdog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenize(lines, token='char')"
      ],
      "metadata": {
        "id": "JE651q0DQ6os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_corpus(tokens):\n",
        "\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "   tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "fbXeemEETCED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "    counter = count_corpus(tokens)\n",
        "\n",
        "    self._token_freqs = sorted(counter.items(), key = lambda x: x[1], reverse=True)\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break;\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "  \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    else:\n",
        "      return [self.to_tokens(idx) for idx in indices]\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    return 0\n",
        "\n",
        "  @property\n",
        "  def token_freqs(self):\n",
        "    return self._token_freqs"
      ],
      "metadata": {
        "id": "cl0mresCRTB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(vocab.token_to_idx.get('t'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7va8W4SVNij",
        "outputId": "c7bbe44f-c549-4f49-807e-a91327acb009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "\n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, 'char')\n",
        "  vocab = Vocab(tokens)\n",
        "\n",
        "  corpus = [vocab[token] for line in tokens for token in line]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens]\n",
        "  return corpus, vocab"
      ],
      "metadata": {
        "id": "6WAi6vKtX56T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSVEkGgRb8Z6",
        "outputId": "a9a9e4d5-cbe9-4d4f-85dd-cbd4a9a96f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170580, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "\n",
        "  corpos = corpus[random.randint(0, num_steps - 1):]\n",
        "  num_subseqs = (len(corpos) - 1) // num_steps\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "    return corpus[pos: pos + num_steps]\n",
        "  \n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(idx) for idx in initial_indices_per_batch]\n",
        "    Y = [data(idx + 1) for idx in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "rbxVbwDgcssn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_seq = list(range(35))\n",
        "\n",
        "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEYkz1mZq6pt",
        "outputId": "f39ac606-2963-4bc3-82f8-2101d67350d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  tensor([[ 5,  6,  7,  8,  9],\n",
            "        [25, 26, 27, 28, 29]]) \n",
            "Y: tensor([[ 6,  7,  8,  9, 10],\n",
            "        [26, 27, 28, 29, 30]])\n",
            "X:  tensor([[20, 21, 22, 23, 24],\n",
            "        [10, 11, 12, 13, 14]]) \n",
            "Y: tensor([[21, 22, 23, 24, 25],\n",
            "        [11, 12, 13, 14, 15]])\n",
            "X:  tensor([[ 0,  1,  2,  3,  4],\n",
            "        [15, 16, 17, 18, 19]]) \n",
            "Y: tensor([[ 1,  2,  3,  4,  5],\n",
            "        [16, 17, 18, 19, 20]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps]\n",
        "    yield X, Y"
      ],
      "metadata": {
        "id": "ISA_wZ3CrVIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZU9q4LzstR8",
        "outputId": "d3a86429-729b-4ea9-b101-e8e6e663d615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  tensor([[ 5,  6,  7,  8,  9],\n",
            "        [19, 20, 21, 22, 23]]) \n",
            "Y: tensor([[ 6,  7,  8,  9, 10],\n",
            "        [20, 21, 22, 23, 24]])\n",
            "X:  tensor([[10, 11, 12, 13, 14],\n",
            "        [24, 25, 26, 27, 28]]) \n",
            "Y: tensor([[11, 12, 13, 14, 15],\n",
            "        [25, 26, 27, 28, 29]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "4IREeXUJsuHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_time_machine(batch_size, num_steps,\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "1doEkdzQtmJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "vWXNuRxHrI0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.token_to_idx.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQOlOQla5V4m",
        "outputId": "2e1636ba-96c9-4ffa-8985-e1ab42d9bf90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('<unk>', 0), (' ', 1), ('e', 2), ('t', 3), ('a', 4), ('i', 5), ('n', 6), ('o', 7), ('s', 8), ('h', 9), ('r', 10), ('d', 11), ('l', 12), ('m', 13), ('u', 14), ('c', 15), ('f', 16), ('w', 17), ('g', 18), ('y', 19), ('p', 20), ('b', 21), ('v', 22), ('k', 23), ('x', 24), ('z', 25), ('j', 26), ('q', 27)])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['t'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lar0PREL5ezq",
        "outputId": "d2874b0b-e6d6-4fa9-e0f7-89aedaa5e010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "\n",
        "  num_inputs = num_outputs = vocab_size\n",
        "  def normal(shape):\n",
        "    return torch.randn(size=shape, device=device) * 0.01\n",
        "\n",
        "  W_xh = normal((num_inputs, num_hiddens))\n",
        "  W_hh = normal((num_hiddens, num_hiddens))\n",
        "  b_h = torch.zeros(num_hiddens, device=device)\n",
        "\n",
        "  W_hq = normal((num_hiddens, num_outputs))\n",
        "  b_q = torch.zeros(num_outputs, device=device)\n",
        "\n",
        "  params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "  for param in params:\n",
        "    param.requires_grad_(True)\n",
        "  \n",
        "  return params"
      ],
      "metadata": {
        "id": "UsnyzcbqrZE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "\n",
        "  return (torch.zeros((batch_size, num_hiddens), device=device), )"
      ],
      "metadata": {
        "id": "AYo7NTGMv8Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(inputs, state, params):\n",
        "\n",
        "  W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "  H, = state\n",
        "  outputs = []\n",
        "\n",
        "  # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "  # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "  for X in inputs:\n",
        "    H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
        "    Y = torch.mm(H, W_hq) + b_q\n",
        "    outputs.append(Y)\n",
        "  return torch.cat(outputs, dim=0), (H,)"
      ],
      "metadata": {
        "id": "qdQ7N6ntwyQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModelScratch:\n",
        "\n",
        "  def __init__(self, vocab_size, num_hiddens, device,\n",
        "                 get_params, init_state, forward_fn):\n",
        "    self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "    self.params = get_params(vocab_size, num_hiddens, device)\n",
        "    self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "  def __call__(self, X, state):\n",
        "\n",
        "    X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "    return self.forward_fn(X, state, self.params)\n",
        "  \n",
        "  def begin_state(self, batch_size, device):\n",
        "    \n",
        "    return self.init_state(batch_size, self.num_hiddens, device)"
      ],
      "metadata": {
        "id": "VmjB2nlNzazc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)"
      ],
      "metadata": {
        "id": "d9mHdVL-1QIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(10).reshape((2, 5))"
      ],
      "metadata": {
        "id": "f-Bg6ehC1hOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = net.begin_state(X.shape[0], try_gpu())"
      ],
      "metadata": {
        "id": "bqagXRjl1ejT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMWPkn_1oZd",
        "outputId": "e2588d71-5964-4145-8236-58623eb20df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y, new_state = net(X.to(try_gpu()), state)"
      ],
      "metadata": {
        "id": "jvz6jKLU1pz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzwE45V81uoG",
        "outputId": "0fc373e4-12f8-4af9-899e-1f7fa4165e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_state[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYx0QBWt18-E",
        "outputId": "d9974f2e-b2fe-4be0-83db-525d6aacfbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
        "\n",
        "  state = net.begin_state(batch_size=1, device= device)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "  for y in prefix[1:]:\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  \n",
        "  for _ in range(num_preds):\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "EfKpPG8n1_KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['m'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajn238Jv4iJD",
        "outputId": "83153c50-b29c-4271-c4cc-ab6986bf5d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ch8('time traveller ', 10, net, vocab, try_gpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8vRYjvTV37sC",
        "outputId": "a3afe8fd-aeb1-46fc-a902-8f98b25f9a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time traveller qzsvkjyilr'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ],
      "metadata": {
        "id": "LAWJD3mJ6jgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "metadata": {
        "id": "BthOeQ7h6wNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "9P2hrFBr60n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()"
      ],
      "metadata": {
        "id": "614dSajw62Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "\n",
        "  state, timer = None, Timer()\n",
        "  metric = Accumulator(2)\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        state.detach_()\n",
        "      else:\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater(batch_size=1)\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "Rl1JUzQL6mhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "  \n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
        "  \n",
        "  predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "\n",
        "  ui_x = []\n",
        "  ui_y = []\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch_ch8(\n",
        "            net, train_iter, loss, updater, device, use_random_iter)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(predict('time traveller'))\n",
        "      ui_x.append(epoch + 1)\n",
        "      ui_y.append(ppl)\n",
        "\n",
        "  \n",
        "  print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))\n",
        "\n",
        "  plt.plot(ui_x, ui_y, 'r')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qWQzdFdwE6s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 0.5\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)"
      ],
      "metadata": {
        "id": "VC9zim5nFKID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ch8(net, train_iter, vocab, lr, num_epochs, try_gpu(),\n",
        "          use_random_iter=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KvU8WVlfFaCf",
        "outputId": "b9ed18b9-59ba-409e-f914-3fede6dc2197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller                                                  \n",
            "time traveller the the the the the the the the the the the the t\n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller the the the the the the the the the the the the t\n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller and the the the the the the the the the the the t\n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time travellerat in the the the the the the the the the the the \n",
            "time traveller and the the the the the the the the the the the t\n",
            "time travellere the andithe andithe andithe andithe andithe andi\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and and the the the the the the the the the the t\n",
            "time traveller and the that and the thall ghat andithe thave the\n",
            "time traveller the the the the the the the the the the the the t\n",
            "time traveller at in thay there this this this this this this th\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller said the medical man the more ard he this the the\n",
            "time traveller sar at in that in the ghist in that is the rime t\n",
            "time traveller of that han stor a dimensions and the three dimen\n",
            "time traveller of the ofrer down sadd thes the thime time ar mer\n",
            "time traveller and and are as ict anof s in thee dimensions ard \n",
            "time traveller cand the thee thing the wime tion at reame andeas\n",
            "time traveller of chatrnestin the mare the rere wime traveller o\n",
            "time traveller and whry foubris the fion and anitalexister at wi\n",
            "time travelleris that is it ustent hime spareller of chepreats b\n",
            "time traveller of chatine sain the mime traveller of chatine sai\n",
            "time traveller ffre bain rfoured whe pay erimenist inttime an an\n",
            "time traveller to dean th easo hor anded ant us the war mengsand\n",
            "time traveller tr ce weling ancassed it yous a dorever this four\n",
            "time traveller fflericed forsper the pals ale atiravedon t mever\n",
            "time traveller after the parsere all atongen stiel thas it ard d\n",
            "time traveller thic aly have io move about in time as ie sis lop\n",
            "time traveller hat sighine thes it lo ked htight one ssmecteanty\n",
            "time traveller but now you begin to seethe object of my inventim\n",
            "time traveller and tha towe so ganklor mo that alint it sthel th\n",
            "time traveller hel sidolexslid fice a machee time sping thick on\n",
            "time traveller but now you begin to seethe object of my investig\n",
            "time traveller arsene than alsmittincteren ad thered the pousthr\n",
            "time traveller for so it will be convenient to speak of himwas e\n",
            "time traveller ar a fall why king the tome thatllle thi here the\n",
            "time traveller perene shis orjpare the torec peonees bou mandane\n",
            "time traveller for so it will be convenient to speak of himwas e\n",
            "time travelleryou can show black is white by argument said filby\n",
            "time traveller held in his hand was a glitteringmetallic framewo\n",
            "time traveller for so it will be convenient to speak of himwas e\n",
            "perplexity 1.1, 20787.0 tokens/sec on cpu\n",
            "time traveller for so it will be convenient to speak of himwas e\n",
            "traveller firee por we dal we hat se oncali scimunelly re f\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
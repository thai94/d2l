{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.6.concise_implementation_of_recurrent_neural_networks_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcdglDZ/FcrHlY92t7DEyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/8.recurrent_neural_networks/8_6_concise_implementation_of_recurrent_neural_networks_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jf02t0SpGzac"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')"
      ],
      "metadata": {
        "id": "4OnQiROhHojZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname"
      ],
      "metadata": {
        "id": "0N4jbVZ4IPlf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)"
      ],
      "metadata": {
        "id": "18M-veC-IQCT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
      ],
      "metadata": {
        "id": "iGLdioAXHlJQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)"
      ],
      "metadata": {
        "id": "JXf5ijLOHijU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "        for token, freq in self._token_freqs:\n",
        "            if freq < min_freq:\n",
        "                break\n",
        "            if token not in self.token_to_idx:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def token_freqs(self):  # Index for the unknown token\n",
        "        return self._token_freqs\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "yw66qI_qHfFy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab"
      ],
      "metadata": {
        "id": "E_ieh-TZHUfu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos: pos + num_steps]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield tf.constant(X), tf.constant(Y)"
      ],
      "metadata": {
        "id": "YAOFUIRtHRlW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = tf.constant(corpus[offset: offset + num_tokens])\n",
        "    Ys = tf.constant(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "    Xs = tf.reshape(Xs, (batch_size, -1))\n",
        "    Ys = tf.reshape(Ys, (batch_size, -1))\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_batches * num_steps, num_steps):\n",
        "        X = Xs[:, i: i + num_steps]\n",
        "        Y = Ys[:, i: i + num_steps]\n",
        "        yield X, Y"
      ],
      "metadata": {
        "id": "0UJnEETGHD-i"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "    \"\"\"An iterator to load sequence data.\"\"\"\n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "        if use_random_iter:\n",
        "            self.data_iter_fn = seq_data_iter_random\n",
        "        else:\n",
        "            self.data_iter_fn = seq_data_iter_sequential\n",
        "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "REUsVD5VHAQv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_time_machine(batch_size, num_steps,\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "9UmpmEMLG_nP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "fHxQ6ZZyG0be"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 256\n",
        "rnn_cell = tf.keras.layers.SimpleRNNCell(num_hiddens,\n",
        "    kernel_initializer='glorot_uniform')\n",
        "\n",
        "rnn_layer = tf.keras.layers.RNN(rnn_cell, time_major=True,\n",
        "    return_sequences=True, return_state=True)\n",
        "\n",
        "state = rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mhzNtKzIk6a",
        "outputId": "7d98a639-6323-4f0c-98b2-63ecfb0a63c3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.random.uniform((num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "Y.shape, state_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5MlT4QwIsPX",
        "outputId": "a94973f9-4c19-43c6-d461-0dd41d362de5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([35, 32, 256]), TensorShape([32, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNNModel, self).__init__(**kwargs)\n",
        "    self.rnn = rnn_layer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, state):\n",
        "\n",
        "    X = tf.one_hot(tf.transpose(inputs), self.vocab_size)\n",
        "    Y, *state = self.rnn(X, state)\n",
        "    output = self.dense(tf.reshape(Y, (-1, Y.shape[-1])))\n",
        "    return output, state\n",
        "  \n",
        "  def begin_state(self, *args, **kwargs):\n",
        "    return self.rnn.cell.get_initial_state(*args, **kwargs)"
      ],
      "metadata": {
        "id": "tqscuCvpJDy3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if len(tf.config.experimental.list_physical_devices('GPU')) >= i + 1:\n",
        "        return tf.device(f'/GPU:{i}')\n",
        "    return tf.device('/CPU:0')\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "    devices = [tf.device(f'/GPU:{i}') for i in range(num_gpus)]\n",
        "    return devices if devices else [tf.device('/CPU:0')]"
      ],
      "metadata": {
        "id": "aQkh1H3nJ9nx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab):\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "    state = net.begin_state(batch_size=1, dtype=tf.float32)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: tf.reshape(tf.constant([outputs[-1]]), (1, 1)).numpy()\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net(get_input(), state)\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net(get_input(), state)\n",
        "        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "c1nQu7odJ5Jg"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = try_gpu()._device_name\n",
        "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
        "with strategy.scope():\n",
        "    net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "\n",
        "predict_ch8('time traveller', 10, net, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vBdprVCyJjqj",
        "outputId": "fea59da2-c4a1-4f1f-ff16-2b0507ea3ac3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time travellersbdqlujvtb'"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(grads, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    theta = tf.constant(theta, dtype=tf.float32)\n",
        "    new_grad = []\n",
        "    for grad in grads:\n",
        "        if isinstance(grad, tf.IndexedSlices):\n",
        "            new_grad.append(tf.convert_to_tensor(grad))\n",
        "        else:\n",
        "            new_grad.append(grad)\n",
        "    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()\n",
        "                        for grad in new_grad))\n",
        "    norm = tf.cast(norm, tf.float32)\n",
        "    if tf.greater(norm, theta):\n",
        "        for i, grad in enumerate(new_grad):\n",
        "            new_grad[i] = grad * theta / norm\n",
        "    else:\n",
        "        new_grad = new_grad\n",
        "    return new_grad"
      ],
      "metadata": {
        "id": "efkTOnlIKrjT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "qKnUo16wLSaU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "metadata": {
        "id": "jcEh6f3OLVAG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):\n",
        "    \"\"\"Train a model within one epoch (defined in Chapter 8).\"\"\"\n",
        "    state, timer = None, Timer()\n",
        "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for X, Y in train_iter:\n",
        "        if state is None or use_random_iter:\n",
        "            # Initialize `state` when either it is the first iteration or\n",
        "            # using random sampling\n",
        "            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n",
        "        with tf.GradientTape(persistent=True) as g:\n",
        "            \n",
        "            y_hat, state = net(X, state)\n",
        "            \n",
        "            y = tf.reshape(tf.transpose(Y), (-1))\n",
        "            l = loss(y, y_hat)\n",
        "        \n",
        "        params = net.trainable_variables\n",
        "        grads = g.gradient(l, params)\n",
        "        \n",
        "        grads = grad_clipping(grads, 1)\n",
        "        \n",
        "        updater.apply_gradients(zip(grads, params))\n",
        "\n",
        "        # Keras loss by default returns the average loss in a batch\n",
        "        # l_sum = l * float(d2l.size(y)) if isinstance(\n",
        "        #     loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
        "        metric.add(l * tf.size(y).numpy(), tf.size(y).numpy())\n",
        "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "WLYAqGylKJex"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, strategy,\n",
        "              use_random_iter=False):\n",
        "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
        "    with strategy.scope():\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        updater = tf.keras.optimizers.SGD(lr)\n",
        "\n",
        "    ui_x = []\n",
        "    ui_y = []\n",
        "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,\n",
        "                                     use_random_iter)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(predict('time traveller'))\n",
        "            ui_x.append(epoch + 1)\n",
        "            ui_y.append(ppl)\n",
        "    device = try_gpu()._device_name\n",
        "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "    print(predict('time traveller'))\n",
        "    print(predict('traveller'))\n",
        "\n",
        "    plt.plot(ui_x, ui_y, 'r')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CnkL-TR1KI_p"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nCurkDqyKA7v",
        "outputId": "9a1cbf21-78b1-422b-9a96-9a379df713ac"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller ou the thing sore the thas an the the the thand t\n",
            "time traveller thit that in the the thes in there the thice is a\n",
            "time travellerthet and her thes in and thas iout tareed the fire\n",
            "time traveller the cad of the s ime same the t an thave is that \n",
            "time traveller the e dimentions and there whack ofoche travely g\n",
            "time traveller thy e mone th tree th th thee thin ther thie sthe\n",
            "time traveller the rid is theno s ang the gimine sove plonght en\n",
            "time travelleryou dan thon ther aid cante in thane three dimensi\n",
            "time traveller this that raver a der the thaved this that nis in\n",
            "time traveller tro eemed iny of he work ald thitk sist rof armat\n",
            "time traveller but red y inc end and this sain thi g ancalnis of\n",
            "time traveller hred in sprepent ang the mithe rave thetoneanoste\n",
            "time traveller s me thit soo thit tomere dion the time time timb\n",
            "time travellerit s against reason sy got an there and rof co din\n",
            "time traveller held in his hand was a glitt filbyct ferithe s th\n",
            "time travellerractly hal theng tore sain the fere wrong wid tey \n",
            "time traveller the eim the fiobe psaven aryald have ingant his f\n",
            "time traveller hall in the dite be the thenot stanotho nt reanel\n",
            "time traveller come ghet sighton whow shewllyou gract and wher a\n",
            "time travellericat our along twe time travellerit sou d me spone\n",
            "time travellericas fow ithts moven t of the thing ssul ofrewty h\n",
            "time travellericullughent ha d the psychologist sookedt stid fil\n",
            "time traveller her aid asmalsthe way halodit ing to be pleres on\n",
            "time travellerit touthe o deef tian foun ward bouscallstollsis t\n",
            "time travelleryou can stof thad fins and the time travellerit s \n",
            "time traveller but newsyon chowe oble scowe cad in an wal it sie\n",
            "time traveller but cepereat y of at abou sthounding thave de tou\n",
            "time traveller but now you teacallen anctlingy us the griver all\n",
            "time traveller pare to the grave dimension wa thes th theme scat\n",
            "time traveller hol coane stht in the time traveller brice asdim \n",
            "time traveller hall in whad bota three time simelabo the e when \n",
            "time traveller pare thee then suthen ther aid taves in y wo cabu\n",
            "time travelleryout abkepti have an asprac in that is thethere he\n",
            "time traveller proceeded anyreal body must havelentto bagacturit\n",
            "time travellerit s agaimst centon shis bege time sallypee at aro\n",
            "time travellerit s against reason said aily inth ta iem im yof t\n",
            "time traveller but now you te in the time arcallext to brectound\n",
            "time traveller fome or viskndly uptedsthe thonghe ffowe chatelle\n",
            "time traveller came back ansavluthesp of ans sermettim the graco\n",
            "time travellerit s against reason said filby but you will spon a\n",
            "time traveller held in his hand was a glitteringmetallic framewo\n",
            "time travellerycre fithry un anof ther vergct and ingerer y tref\n",
            "time travellerit s against reasleall scingit bove anous i s all \n",
            "time travellerit s against geasato and hardistee andeand and how\n",
            "time traveller proceeded anyreal body must have extension in fou\n",
            "time travellericat focr helrid of at un the reamo lbrge thit is \n",
            "time traveller proceeded anyme somptain the fooll so four have t\n",
            "time traveller hand de his shat uparksus iftling abaty mali fous\n",
            "time traveller after the pauserequired for ind ane toreceatthous\n",
            "time traveller proceeded anyour that beeme momedaice uronet s al\n",
            "perplexity 1.4, 15820.5 tokens/sec on /GPU:0\n",
            "time traveller proceeded anyour that beeme momedaice uronet s al\n",
            "traveller ffrentw on a fopust tha fole hbur dou ef aim ti b\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
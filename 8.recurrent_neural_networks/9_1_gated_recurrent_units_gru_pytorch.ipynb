{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.1.\u001dgated_\u001drecurrent_units_gru_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxl4ZTtY68TFwJ1jPPzkhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thai94/d2l/blob/main/8.recurrent_neural_networks/9_1_%1Dgated_%1Drecurrent_units_gru_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pYrLC_lx10Yf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "import re\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, 'Only zip/tar files can be extracted.'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():\n",
        "    \"\"\"Download all files in the DATA_HUB.\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)\n",
        "\n",
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def tokenize(lines, token='word'):\n",
        "\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == 'char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print('ERROR: unknow token type: ' + token)\n",
        "\n",
        "def count_corpus(tokens):\n",
        "\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "   tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "    counter = count_corpus(tokens)\n",
        "\n",
        "    self._token_freqs = sorted(counter.items(), key = lambda x: x[1], reverse=True)\n",
        "    self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "    for token, freq in self._token_freqs:\n",
        "      if freq < min_freq:\n",
        "        break;\n",
        "      if token not in self.token_to_idx:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "  \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    else:\n",
        "      return [self.to_tokens(idx) for idx in indices]\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    return 0\n",
        "\n",
        "  @property\n",
        "  def token_freqs(self):\n",
        "    return self._token_freqs\n",
        "\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "\n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, 'char')\n",
        "  vocab = Vocab(tokens)\n",
        "\n",
        "  corpus = [vocab[token] for line in tokens for token in line]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens]\n",
        "  return corpus, vocab\n",
        "\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "\n",
        "  corpos = corpus[random.randint(0, num_steps - 1):]\n",
        "  num_subseqs = (len(corpos) - 1) // num_steps\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "    return corpus[pos: pos + num_steps]\n",
        "  \n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(idx) for idx in initial_indices_per_batch]\n",
        "    Y = [data(idx + 1) for idx in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps]\n",
        "    yield X, Y\n",
        "\n",
        "class SeqDataLoader:\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "def load_data_time_machine(batch_size, num_steps,\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab\n",
        "\n"
      ],
      "metadata": {
        "id": "m_dRvGG22kLz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "SYAoJZYI198b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "\n",
        "  num_inputs = num_outputs = vocab_size\n",
        "  def normal(shape):\n",
        "    return torch.randn(size=shape, device=device)*0.01\n",
        "\n",
        "  def three():\n",
        "    return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device))\n",
        "\n",
        "  W_xz, W_hz, b_z = three()\n",
        "  W_xr, W_hr, b_r = three()\n",
        "  W_xh, W_hh, b_h = three()\n",
        "\n",
        "  W_hq = normal((num_hiddens, num_outputs))\n",
        "  b_q = torch.zeros(num_outputs, device=device)\n",
        "\n",
        "  params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
        "  for param in params:\n",
        "    param.requires_grad_(True)\n",
        "  return params"
      ],
      "metadata": {
        "id": "0RUeBc8u3Wne"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_gru_state(batch_size, num_hiddens, device):\n",
        "\n",
        "  return (torch.zeros((batch_size, num_hiddens), device=device), )"
      ],
      "metadata": {
        "id": "gooaxoMF6rBC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gru(inputs, state, params):\n",
        "\n",
        "  W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "\n",
        "  H, = state\n",
        "  outputs = []\n",
        "\n",
        "  for X in inputs:\n",
        "\n",
        "    Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
        "    R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
        "\n",
        "    H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
        "\n",
        "    H = Z * H + (1 - Z) * H_tilda\n",
        "\n",
        "    Y = H @ W_hq + b_q\n",
        "    outputs.append(Y)\n",
        "\n",
        "  return torch.cat(outputs, dim=0), (H,)"
      ],
      "metadata": {
        "id": "y_soSn586vKu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModelScratch:\n",
        "\n",
        "  def __init__(self, vocab_size, num_hiddens, device,\n",
        "                 get_params, init_state, forward_fn):\n",
        "    self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "    self.params = get_params(vocab_size, num_hiddens, device)\n",
        "    self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "  def __call__(self, X, state):\n",
        "\n",
        "    X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "    return self.forward_fn(X, state, self.params)\n",
        "  \n",
        "  def begin_state(self, batch_size, device):\n",
        "    \n",
        "    return self.init_state(batch_size, self.num_hiddens, device)\n",
        "\n",
        "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
        "\n",
        "  state = net.begin_state(batch_size=1, device= device)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "  for y in prefix[1:]:\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  \n",
        "  for _ in range(num_preds):\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
        "\n",
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()\n",
        "\n",
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "\n",
        "  state, timer = None, Timer()\n",
        "  metric = Accumulator(2)\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        state.detach_()\n",
        "      else:\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater(batch_size=1)\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
        "\n",
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "  \n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
        "  \n",
        "  predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "\n",
        "  ui_x = []\n",
        "  ui_y = []\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch_ch8(\n",
        "            net, train_iter, loss, updater, device, use_random_iter)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(predict('time traveller'))\n",
        "      ui_x.append(epoch + 1)\n",
        "      ui_y.append(ppl)\n",
        "\n",
        "  \n",
        "  print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))\n",
        "\n",
        "  plt.plot(ui_x, ui_y, 'r')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iqkJHUDl7ebB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, num_hiddens, device = len(vocab), 256, try_gpu()\n",
        "num_epochs, lr = 500, 1\n",
        "model = RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
        "                            init_gru_state, gru)"
      ],
      "metadata": {
        "id": "aJbHLlBj7WGm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7tznkjSI7a4r",
        "outputId": "674c96ae-b274-47c4-ce97-2aef5fd696c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller                                                  \n",
            "time traveller at at at at at at at at at at at at at at at at a\n",
            "time travellere the the the the the the the the the the the the \n",
            "time travellere the the the the the the the the the the the the \n",
            "time travellere the the the the the the the the the the the the \n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller an the the the the the the the the the the the th\n",
            "time travellere the the the the the the the the the the the the \n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time travellere the the the the the the the the the the the the \n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller the the the the the the the the the the the the t\n",
            "time traveller and the the the the the the the the the the the t\n",
            "time traveller the the the the the the the the the the the the t\n",
            "time traveller the the the the the the the the the the the the t\n",
            "time traveller the thing the thing the thing the thing the thing\n",
            "time traveller there and the thing the thing the thing the thing\n",
            "time traveller there and there and there and there and there and\n",
            "time traveller the thing the thing the thing the thing the thing\n",
            "time traveller the thing the thick in the thing the thick in the\n",
            "time traveller the time traveller the time traveller the time tr\n",
            "time traveller the medical man a might and the medical man a mig\n",
            "time traveller the promet in the menter the mensions and why on \n",
            "time traveller three dimensions we that is a said the time trave\n",
            "time traveller threed the mentare the three dimensions three dim\n",
            "time traveller the person ind man the time traveller the pentaci\n",
            "time traveller the promet you anothem that is the time traveller\n",
            "time traveller the periontif the exom the gromet dimensions of t\n",
            "time traveller the persontime travel inttonthree i bughered sain\n",
            "time traveller and so erame travel traveller the wendessing alon\n",
            "time travelleryou can show veet calo mathed is and to easter the\n",
            "time traveller thepersunness really this is what is meant byttre\n",
            "time traveller wht ancubutey but so crave to sayd and thating si\n",
            "time traveller the pardityed in one direction alongthe time dime\n",
            "time traveller the prover downingtith all have to stating his fo\n",
            "time traveller hepr and sut i waid t you they u ungeg all abatt \n",
            "time travellerit w acknystigns of himway macount limvestan sur g\n",
            "time traveller the parsit efultith maitht all in thisbounn wille\n",
            "time traveller the pardityed it inee free you oncowt rogest are \n",
            "time travelleryou can show black is white by argument said filby\n",
            "time traveller but now you begin to seethe object of my investig\n",
            "time travelleryou can show black is white by argument said filby\n",
            "time traveller the meriment said filby but you willnever convinc\n",
            "time traveller for so it will be convenient to speak of himwas e\n",
            "time traveller with a slight accession ofcheerfulness really thi\n",
            "time traveller with a slight accession ofcheerfulness really thi\n",
            "time travelleryou can show black is white by argument said filby\n",
            "perplexity 1.1, 34117.9 tokens/sec on cuda:0\n",
            "time travelleryou can show black is white by argument said filby\n",
            "travelleryou can show black is white by argument said filby\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD8CAYAAABuHP8oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdrklEQVR4nO3deZhU1ZnH8e9LN4sgiECLDs0ighDgQYQWUNEoikHDiCQq4obABDGJ4hYDcdRJZuIWlbglioHoTBQXhBB3WRR3tEEUUFBAkE1oUVBxQeSdP061tC3YTS1961b9Ps9TT1XdKrrei+2vDueexdwdERGJn1pRFyAiIslRgIuIxJQCXEQkphTgIiIxpQAXEYkpBbiISExVGeBmNtHMNpjZwgrHupnZK2Y238xKzaxnZssUEZHKqtMCvxvoX+nY9cDv3b0bcGXiuYiI1KAqA9zdnwM+qnwYaJR4vBewNs11iYhIFQqT/HMXAk+Z2Q2EL4HDqvOHmjVr5m3atEnyI0VE8tPcuXM/dPeiyseTDfDzgIvc/WEzOxWYABy7szea2UhgJECrVq0oLS1N8iNFRPKTma3c2fFkR6EMBaYkHj8E7PIipruPd/cSdy8pKvreF4iIiCQp2QBfC/w48bgv8G56yhERkeqqsgvFzCYBRwHNzGw1cBXwC+BmMysEviTRRSIiIjWnygB39yG7eKlHmmsREZHdoJmYIiIxpQAXEYkpBbiISEzFI8CffBKuvTbqKkREsko8AnzmTLjqKvj886grERHJGvEI8H79YOtWeO65qCsREcka8QjwI46AunVh+vSoKxERyRrxCPA99oA+fRTgIiIVxCPAAY47DhYsgHXroq5ERCQrxCfA+/UL9zNmRFuHiEiWiE+AH3QQFBWpG0VEJCE+AV6rFhxzTGiBu0ddjYhI5OIT4BC6Udatg0WLoq5ERCRy8QtwUDeKiAhxC/CWLaFDBwW4iAhxC3AIrfDZs+Grr6KuREQkUlUGuJlNNLMNZraw0vHzzWyxmS0ys+szV2Il/fqFNVFefrnGPlJEJBtVpwV+N9C/4gEzOxoYCBzk7p2BG9Jf2i4cdRQUFKgbRUTyXpUB7u7PAR9VOnwecK27f5V4z4YM1LZzjRpB794KcBHJe8n2gR8IHGFmc8xstpkdsqs3mtlIMys1s9KysrIkP66Sfv2gtBQ2bkzPzxMRiaFkA7wQaAL0Bn4DPGhmtrM3uvt4dy9x95KioqIkP66Sfv3CZJ5Zs9Lz80REYijZAF8NTPHgVWA70Cx9ZVWhZ8/QlaJuFBHJY8kG+D+BowHM7ECgDvBhuoqqUmEh9O0bAlzT6kUkT1VnGOEk4GWgg5mtNrMRwESgbWJo4f3AUPcaTtJ+/WDFCli2rEY/VkQkWxRW9QZ3H7KLl85Mcy27p+K0+nbtIi1FRCQK8ZuJWa5dO2jdWv3gIpK34hvgZqEVPmsWbNsWdTUiIjUuvgEO4ULm5s1hqzURkTwT7wAvKQn3r78ebR0iIhGId4AfcAA0bAhz50ZdiYhIjYt3gNeqBd27w7x5UVciIlLj4h3gEAL8jTd0IVNE8k5uBPgXX8CSJVFXIiJSo3IjwEHdKCKSd+If4B06wB576EKmiOSd+Ad4QQF066YWuIjknfgHOECPHmEs+PbtUVciIlJjciPAu3eHzz6DpUujrkREpMbkToCDulFEJK/kRoB36gR16uhCpojkleps6DDRzDYkNm+o/NolZuZmVnPbqe1M7drQtata4CKSV6rTAr8b6F/5oJm1BI4D3k9zTcnp0SMEuLZYE5E8UWWAu/tzwEc7eWkccBmQHYnZvTts2hS2WRMRyQNJ9YGb2UBgjbu/keZ6kld+IVP94CKSJ3Y7wM2sPvA74Mpqvn+kmZWaWWlZWdnuflz1dekSdqtXP7iI5IlkWuAHAPsDb5jZCqAYmGdm++7sze4+3t1L3L2kqKgo+UqrUq8edO6sABeRvFHlrvSVufsCYJ/y54kQL3H3D9NYV3J69IBHHgkXMs2irkZEJKOqM4xwEvAy0MHMVpvZiMyXlaTu3aGsDNasiboSEZGMq7IF7u5Dqni9TdqqSVXFC5nFxdHWIiKSYbkxE7Nc165hmzX1g4tIHsitAG/QADp2VICLSF7IrQAHbXIsInkj9wK8Rw9YuxY++CDqSkREMir3AlxLy4pInsi9AO/WLdwrwEUkx+VegDdqBO3bK8BFJOflXoCDLmSKSF7IzQDv0QNWroSNG6OuREQkY3IzwMsvZJaWRluHiEgG5WaA9+oF9evD5MlRVyIikjG5GeB77gmnngr33w9btkRdjYhIRuRmgAMMHw6ffQYPPxx1JSIiGZG7Ad6nD7RrBxMnRl2JiEhG5G6Am8GwYTB7NixbFnU1IiJpl7sBDnD22WF52bvvjroSEZG0q86OPBPNbIOZLaxw7E9mttjM3jSzqWbWOLNlJqm4GI47LgT4N99EXY2ISFpVpwV+N9C/0rHpQBd37wq8A4xNc13pM3w4rF4NM2ZEXYmISFpVGeDu/hzwUaVjT7v7tsTTVwg702enE0+EJk3g73+PuhIRkbRKRx/4cOCJXb1oZiPNrNTMSsvKytLwcbupbl044wyYOhU++qjq94uIxERKAW5mlwPbgHt39R53H+/uJe5eUlRUlMrHJW/4cNi6Fe67L5rPFxHJgKQD3MzOAQYAZ7i7p62iTOjWDQ4+WN0oIpJTkgpwM+sPXAac6O6fp7ekDBk2LCwxO39+1JWIiKRFdYYRTgJeBjqY2WozGwHcBjQEppvZfDO7I8N1pu7006FOHbXCRSRnWE32fpSUlHhplEu8Dh4MM2fCmjXh4qaISAyY2Vx3L6l8PLdnYlY2bFjY5GHatKgrERFJWX4FeL9+YYGrSy/Vbj0iEnv5FeAFBWGN8PXr4cwzYfv2qCsSEUlafgU4hP0yb74ZnnwSrr466mpERJKWfwEOcO65YVTKVVfBrFlRVyMikpT8DHAzuPNO6NABhgyBtWujrkhEZLflZ4BD2Ddz8uSw7dppp8G2bVX/GRGRLJK/AQ7QqRPcdRc8/zxcfnnU1YiI7Jb8DnAIfeGjRsH112sDZBGJFQU4wLhxUFICJ58cJvts2BB1RSIiVVKAA9SrB88+C2PGwL33houbt9+ubdhEJKspwMs1aADXXANvvhnGiv/613DIIfDyy1FXJiKyUwrwyjp2hOnT4YEHwozNww6D887TKBURyToK8J0xg1NPhcWL4aKL4I47wq4+mnovIlmkMOoCslrDhnDTTWFT5CuuCGPHb789BLyISMSqDHAzm0jYOm2Du3dJHGsCPAC0AVYAp7r7x5krM2KXXw6ffhqGGjZsCNdeqxAXkchVpwvlbqB/pWNjgJnu3h6YmXieu8xCaP/ylyHE//jHqCsSEam6Be7uz5lZm0qHBwJHJR7fAzwL/DaNdWUfM7j11jD1vrw75cILo65KRPJYsn3gzd19XeLxB0DzNNWT3WrVggkTYMuWcHGzYUMYMSLqqkQkT6U8CsXDppq73FjTzEaaWamZlZaVlaX6cdErLIT77oP+/eEXvwi7+3zxRdRViUgeSjbA15vZfgCJ+13OPXf38e5e4u4lRUVFSX5clqlTB6ZMgZEj4cYb4eCDYc6cqKsSkTyTbID/CxiaeDwUyL9dgvfYI4wPf/pp+PzzMOFnzBj48suoKxORPFFlgJvZJOBloIOZrTazEcC1QD8zexc4NvE8P/XrBwsWhIk+110XpuGXlkZdlYjkgSoD3N2HuPt+7l7b3YvdfYK7b3T3Y9y9vbsf6+4f1USxWWuvvcK64o8/Dps3Q+/ecNZZoVvFd3l5QEQkJZpKn07HHw8LF4aFsKZNC0Hesyfcc4+6VkQk7RTg6da4Mfz5z7BmTZh2v2ULnHMOFBfD2LGwalXUFYpIjlCAZ0rDhmHm5qJFMHMmHHlkmMXZtm3YNGLx4qgrFJGYU4Bnmhn07RuGHS5fHpamfeCBsB/nz34Gr74adYUiElMK8JrUujXccgusXAn/+Z/wzDPQqxcccwy8+GLU1YlIzCjAo1BUBH/4A7z/fpgItHgxHH00PPZY1JWJSIwowKPUsCFcfHHoJ+/aNXSpPPVU1FWJSEwowLNB48ZhRmenTjBwIMyYEXVFIhIDCvBs0aRJCO4OHeDf/x1mzYq6IhHJcgrwbNK0aQjxAw4IIT57dtQViUgWU4Bnm6KiMG68dWv46U/hhReirkhEspQCPBs1bx5CvEULOPZY+NWvYMWKqKsSkSyjAM9W++0XulDOOisslNWuHQwdCm+9FXVlIpIlFODZbN99Q3gvXw7nnw+TJ0PnzmG44WuvRV2diERMAR4HxcUwblyYwXnFFWEGZ8+ecOaZ8MEHUVcnIhFJKcDN7CIzW2RmC81skpnVS1dhshPNmoUZnOVB/tBDYdjhrbfCtm1RVyciNSzpADezFsAFQIm7dwEKgNPSVZj8gEaNQpAvXBjWHL/ggtAif+WVqCsTkRqUahdKIbCHmRUC9YG1qZck1da+PTz5JDz4IKxfD4ceGjZa3rDLPaZFJIckHeDuvga4AXgfWAdsdven01WYVJMZnHJKWBDr4oth4kRo0wYuugjW6vtUJJel0oWyNzAQ2B/4N6CBmZ25k/eNNLNSMystKytLvlL5YQ0bhpUNFy0KgX7rrbD//mFTiZUro65ORDIglS6UY4H33L3M3b8GpgCHVX6Tu4939xJ3LykqKkrh46RaOnQIe3C+804YN/63v4Ux5P/xH/Duu1FXJyJplEqAvw/0NrP6ZmbAMcDb6SlLUta2LYwfD0uXwqhR8I9/hHD/2c/C5hHuUVcoIilKpQ98DjAZmAcsSPys8WmqS9KlVavQnbJiBfzud2F2Z58+4YLn5MnwzTdRVygiSUppFIq7X+XuHd29i7uf5e5fpaswSbN994X/+Z+wC9Btt8GHH4a+8vbtQ0t9+/aoKxSR3aSZmPmmQYOwONaSJWGj5ebN4dxzw76c770XdXUishsU4PmqoAAGDYKXXgoXOufODdu63Xmn+sdFYkIBnu/MYMSIHbM6R42C/v1h1aqoKxORKijAJWjVKuzL+Ze/hFEqXbqESUHqGxfJWgpw2cEMzjsP3nwTunULLfNu3WDqVHWriGQhBbh8X9u2YcnaSZNg69YwdrxHD3j0UQW5SBZRgMvO1aoFp50W+sbvuQc2bw4bLffuDU89pSAXyQIKcPlhhYVw9tlhsay77gobSPTvDz//uTaTEImYAlyqp3btHeupXHcdPP542N7tvvvUGheJiAJcdk+dOnDZZfD662EW5xlnhPHk69ZFXZlI3lGAS3J+9KMw3PBPfwp94p07hwWz1BoXqTEKcEleQQFceinMnw8dO8JZZ4Up+XPmRF2ZSF5QgEvqOnSA558Pqx6Wz+gcNChsLiEiGaMAl/QoKIBf/xqWLQsbLs+cGdZWOecc7QgkkiEKcEmvhg3hiitg+fKwL+f998OBB4YVEJcvj7o6kZyiAJfMaNYMbrghDDscOjSMIW/fHoYMCSNYRCRlKQW4mTU2s8lmttjM3jazQ9NVmOSIli3DhhHvvQeXXAKPPQbdu8Nxx8GMGRq1IpKCVFvgNwNPuntH4CC0J6bsSosWcP31YUega6+FBQugXz/o2RNmzYq6OpFYSjrAzWwv4EhgAoC7b3X3TekqTHJU48bw29+GPTrvugs2bAhDD48/PqyCKCLVlkoLfH+gDPi7mb1uZn8zswZpqktyXd26YWr+kiVhMtArr4Sla4cN02YSItWUSoAXAt2Bv7r7wcAWYEzlN5nZSDMrNbPSsrKyFD5OclK9emEy0LJloY980qQwamXsWPjii6irE8lqqQT4amC1u5dPu5tMCPTvcPfx7l7i7iVFRUUpfJzktCZNQkt8yRI45ZTQT96rF7z1VtSViWStpAPc3T8AVplZh8ShYwD93yapad0a/vd/4cknYf16KCkJfeUarSLyPamOQjkfuNfM3gS6AVenXpII8JOfwBtvwOGHw8iRMHgwbNI1cpGKUgpwd5+f6B7p6u4nufvH6SpMhH33DSsdXnMNTJkCBx8cLnaKCKCZmJLtatWCMWPghRfC8z59YPRo0AVxEQW4xETv3mEK/vDhcNttYePl3/8ePv006spEIqMAl/ho3DhMy1+0KEzF/6//ggMOCMvYbt0adXUiNU4BLvHTsSM8/HDoD+/cGS64IBx77LGoKxOpUQpwia9evcI6Kk88AQ0awIABcPHFao1L3lCAS7yZQf/+8NprYUOJcePC0MNly6KuTCTjFOCSG+rVC33hU6bA0qVhydoHH4y6KpGMUoBLbhk0KGyy3KlTmPxz7rlaU0VylgJcck/r1vDcc2HZ2vHjw96ckydrOr7kHAW45KbatcOCWNOnh6VrTzkFDjsMnn8+6spE0kYBLrnt2GPDmioTJoR1xo88EgYOhLe1eZTEnwJccl9BQZjB+c47cPXV8Oyz0KUL/PKX6h+XWFOAS/6oXz9sFLFsWRhyeMcd8OMfw9q1UVcmkhQFuOSfZs3g5pth6tSwYcQhh0BpadRView2Bbjkr4ED4aWXwgXPI46ABx6IuiKR3ZJygJtZQWJT40fTUZBIjeraFV59Nez8c9ppcOWVsH171FWJVEs6WuCjAV3Sl/jaZx+YMQOGDYP//u8w5PBj7U0i2S+lADezYuCnwN/SU45IROrWDUMNb7wR/vlP+NGPQpeKJv9IFku1Bf5n4DJA/+aU+DMLqxm+9hoUF4culQEDYOXKqCsT2amkA9zMBgAb3H1uFe8baWalZlZapm2wJA66dw9rjd90E8yeHdZVGTcOtm2LujKR70ilBX44cKKZrQDuB/qa2T8qv8ndxyc2Pi4pKipK4eNEalBhIVx0Udj95+ijQ8u8Vy8NN5SsknSAu/tYdy929zbAacAsdz8zbZWJZIPWreGRR8LStGvXQs+eYYXDjRujrkxE48BFqmQWRqYsWQIXXhgudh54INx5J3zzTdTVSR5LS4C7+7PuPiAdP0skazVqFPrF588Pa6mMGhW6VebMiboyyVNqgYvsri5dwoJY994bulV694bRo+Grr6KuTPKMAlwkGWZw+umweHFYGOuWW6BPH3jvvagrkzyiABdJRaNGO/bifPfdMARx2rSoq5I8oQAXSYdBg2DePGjbFk46CS69FL7+OuqqJMcpwEXSpW1bePHFsFHEjTeGtcZXrYq6KslhCnCRdKpXD26/He6/HxYuDF0qs2dHXZXkKAW4SCYMHhzWVGnaNOzL+de/amEsSTsFuEimdOgQxoj/5CehW2XUKNi6NeqqJIcowEUyaa+9wqiUsWNh/Hjo2xfWr4+6KskRCnCRTCsogKuvDv3i8+aF3X/m/uAiniLVogAXqSmDB8MLL4RJQL16hbXGH35Y3SqSNAW4SE3q3j20vn/zG3j9dTj5ZGjRIiyS9cYbUVcnMaMAF6lpRUVwzTVhp5/HH4ejjoK//AW6dQvdK/PmRV2hxIQCXCQqhYVw/PHw0EOwbl1YT2X9+rCmykMPRV2dxIACXCQbNG0K558fdvzp1g1OPRWuugq2a7tZ2TUFuEg2ad4cnnkGzjkH/vCHEORbtkRdlWSpVDY1bmlmz5jZW2a2yMxGp7MwkbxVty5MnBjWU5k6FQ4/PPSXi1SSSgt8G3CJu3cCegO/MrNO6SlLJM+ZhY2UH300rDF+yCEwaRJs2xZ1ZZJFUtnUeJ27z0s8/hR4G2iRrsJEhHCRc86c0LVy+unQvn1Yf1zdKkKa+sDNrA1wMPC9zQHNbKSZlZpZaVlZWTo+TiS/dOwYxohPmwbFxXDBBdCqFVxxBWzYEHV1EqGUA9zM9gQeBi50908qv+7u4929xN1LioqKUv04kfxUqxaceCI8/zy89FJYa/yPfwxBPnx4OKbVDvNOSgFuZrUJ4X2vu09JT0ki8oMOPTRs4bZ4MQwbFsaMH344dOoULnyqVZ43UhmFYsAE4G13vyl9JYlItRx4YFhnfN06mDABmjQJW7m1aAE//zk89pi2dctxqbTADwfOAvqa2fzE7YQ01SUi1bXnnqEb5cUX4a23YPTo0NUyYADst19Yh3z2bE0KykHmNdhvVlJS4qWlpTX2eSJ5a+tWeOqpMPRw2jT4/PPQMh88OLTOO3WCxo2jrlKqyczmunvJ944rwEVy3JYt8MgjIcyfeGJHt8ree8P++4fNmNu2hXbt4IQTQtBLVlGAiwh8/DE8+ywsX/7d24oVodVuFvbwHDoUTjoJGjSIumJh1wFeGEUxIhKRvfeGQYO+f3z7dnjnHbjvPvi//4Mzzwx96yefHMK8pCSEuVnN1yy7pBa4iHzX9u1h56B77glDFD/9NByvXTusmtikSbg1bRpmiBYXQ8uW371Xyz2t1IUiIrvv88/DphPvvQcffQQbN4b78scffLDzcecNG4bWfuPG4Vb+uH790Cf/ySeweXO4/+QT+OyzsNFFq1bh1rr1jsf164dJStu3f/e2bRt89VW4ffnljsdmsM8+4ctl333D49q1d++8t2/fsdVd7dphX9Mf8s034dpC+Vo15f9SMdtxq87P2QV1oYjI7qtfP3Sj/JAvv4S1a2HVKli9OtzWrYNNm3bcVqwI9599FrpmGjUKt+bNw/ou9euHL4L33w+t/02b0nseTZpAs2YhSCt/EZSH79at4Qtg69bvLxpmBnXqhBCuXTs8L/8zX39dvSGaTzwB/fun9bQU4CKSmnr1doxkSZdPPglfCO+/v6NVXavWjptZ2NGobt1wq1dvx+Pt28OXwfr14V8I69eH24cffv/nlP+sOnXCn614X6dOqOXrr3fcygPbfUegV7yv2MJ233GDMPEqzRTgIpJ9GjWCzp3DLRnp/DLJYtqRR0QkphTgIiIxpQAXEYkpBbiISEwpwEVEYkoBLiISUwpwEZGYUoCLiMRUja6FYmZlwMoq3tYM+LAGysk2Ou/8ovPOP6mce2t3/96u8DUa4NVhZqU7W7Ql1+m884vOO/9k4tzVhSIiElMKcBGRmMrGAB8fdQER0XnnF513/kn7uWddH7iIiFRPNrbARUSkGrImwM2sv5ktMbOlZjYm6nrSzcwmmtkGM1tY4VgTM5tuZu8m7vdOHDczuyXxd/GmmXWPrvLkmVlLM3vGzN4ys0VmNjpxPKfPG8DM6pnZq2b2RuLcf584vr+ZzUmc4wNmVidxvG7i+dLE622irD8VZlZgZq+b2aOJ5zl/zgBmtsLMFpjZfDMrTRzL6O96VgS4mRUAtwPHA52AIWbWKdqq0u5uoPJ+SmOAme7eHpiZeA7h76F94jYS+GsN1Zhu24BL3L0T0Bv4VeK/a66fN8BXQF93PwjoBvQ3s97AdcA4d28HfAyMSLx/BPBx4vi4xPviajTwdoXn+XDO5Y52924Vhgtm9nfd3SO/AYcCT1V4PhYYG3VdGTjPNsDCCs+XAPslHu8HLEk8vhMYsrP3xfkGTAP65eF51wfmAb0IEzkKE8e//b0HngIOTTwuTLzPoq49iXMtTgRVX+BRwHL9nCuc+wqgWaVjGf1dz4oWONACWFXh+erEsVzX3N3XJR5/ADRPPM65v4/EP48PBuaQJ+ed6EqYD2wApgPLgE3uXr5jbsXz+/bcE69vBprWbMVp8WfgMqB8l9+m5P45l3PgaTOba2YjE8cy+ruuPTGzhLu7meXkkCAz2xN4GLjQ3T8xs29fy+XzdvdvgG5m1hiYCnSMuKSMMrMBwAZ3n2tmR0VdTwT6uPsaM9sHmG5miyu+mInf9Wxpga8BWlZ4Xpw4luvWm9l+AIn7DYnjOfP3YWa1CeF9r7tPSRzO+fOuyN03Ac8Qug8am1l5w6ni+X177onX9wI21nCpqTocONHMVgD3E7pRbia3z/lb7r4mcb+B8IXdkwz/rmdLgL8GtE9cra4DnAb8K+KaasK/gKGJx0MJfcTlx89OXKnuDWyu8M+w2LDQ1J4AvO3uN1V4KafPG8DMihItb8xsD0Lf/9uEID858bbK517+d3IyMMsTnaNx4e5j3b3Y3dsQ/h+e5e5nkMPnXM7MGphZw/LHwHHAQjL9ux51x3+FTvwTgHcI/YSXR11PBs5vErAO+JrQ3zWC0N83E3gXmAE0SbzXCKNylgELgJKo60/ynPsQ+gXfBOYnbifk+nknzqUr8Hri3BcCVyaOtwVeBZYCDwF1E8frJZ4vTbzeNupzSPH8jwIezZdzTpzjG4nbovIMy/TvumZiiojEVLZ0oYiIyG5SgIuIxJQCXEQkphTgIiIxpQAXEYkpBbiISEwpwEVEYkoBLiISU/8PcdYjGHEj3xQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}